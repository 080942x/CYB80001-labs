{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYB80001 System Security Project\n",
    "Prepared by **Derui (Derek) Wang**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3D - Carlini&Wagner Attack and Defences\n",
    "\n",
    "In this session, we will first implement the **Carlini & Wagner(C&W) attack** based on **Tensorflow** and **Keras**. We will then apply the attacks to a Keras classifier to evaluate the attacks.\n",
    "\n",
    "Second, we will implement adversarial training as a defence on the Keras classifier. We will test the effectiveness of the defence against C&W attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "\n",
    "### Part 1 Implementing the C&W attack\n",
    "\n",
    "1.1 [Implementing C&W](#1_1)\n",
    "\n",
    "1.2 [Testing C&W](#1_2)\n",
    "\n",
    "\n",
    "### Part 2 Adversarial training as a defence\n",
    "\n",
    "2.1 [Implementing adversarial training](#2_1)\n",
    "\n",
    "2.2 [Black-box C&W attacks against adversarial training](#2_2)\n",
    "\n",
    "2.3 [White-box C&W attacks against adversarial training](#2_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1. Implementing the C&W attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"1_1\"></a>\n",
    "\n",
    "## 1.1  Implementing C&W\n",
    "\n",
    "We will implement the **C&W attack** in this part. For reusing the C&W code in the following parts, we will implement it as a Python **class** object. \n",
    "\n",
    "We first import required packages and define hyper-parameters of the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2    # larger values converge faster to less accurate results\n",
    "TARGETED = True          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0           # how strong the adversarial example should be\n",
    "INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the C&W attack as a Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarliniL2:\n",
    "    def __init__(self, sess, model, batch_size=1, confidence = CONFIDENCE,\n",
    "                 targeted = TARGETED, learning_rate = LEARNING_RATE,\n",
    "                 binary_search_steps = BINARY_SEARCH_STEPS, max_iterations = MAX_ITERATIONS,\n",
    "                 abort_early = ABORT_EARLY, \n",
    "                 initial_const = INITIAL_CONST,\n",
    "                 boxmin = 0, boxmax = 1):\n",
    "        \"\"\"\n",
    "        The L_2 optimized attack. \n",
    "\n",
    "        This attack is the most efficient and should be used as the primary \n",
    "        attack to evaluate potential defenses.\n",
    "\n",
    "        Returns adversarial examples for the supplied model.\n",
    "\n",
    "        confidence: Confidence of adversarial examples: higher produces examples\n",
    "          that are farther away, but more strongly classified as adversarial.\n",
    "        batch_size: Number of attacks to run simultaneously.\n",
    "        targeted: True if we should perform a targetted attack, False otherwise.\n",
    "        learning_rate: The learning rate for the attack algorithm. Smaller values\n",
    "          produce better results but are slower to converge.\n",
    "        binary_search_steps: The number of times we perform binary search to\n",
    "          find the optimal tradeoff-constant between distance and confidence. \n",
    "        max_iterations: The maximum number of iterations. Larger values are more\n",
    "          accurate; setting too small will require a large learning rate and will\n",
    "          produce poor results.\n",
    "        abort_early: If true, allows early aborts if gradient descent gets stuck.\n",
    "        initial_const: The initial tradeoff-constant to use to tune the relative\n",
    "          importance of distance and confidence. If binary_search_steps is large,\n",
    "          the initial constant is not important.\n",
    "        boxmin: Minimum pixel value (default -0.5).\n",
    "        boxmax: Maximum pixel value (default 0.5).\n",
    "        \"\"\"\n",
    "\n",
    "        image_size, num_channels, num_labels = model.image_size, model.num_channels, model.num_labels\n",
    "        self.sess = sess\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.BINARY_SEARCH_STEPS = binary_search_steps\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.CONFIDENCE = confidence\n",
    "        self.initial_const = initial_const\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.repeat = binary_search_steps >= 10\n",
    "\n",
    "        shape = (batch_size,image_size,image_size,num_channels)\n",
    "        \n",
    "        # the variable we're going to optimize over\n",
    "        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "\n",
    "        # these are variables to be more efficient in sending data to tf\n",
    "        self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32)\n",
    "        self.tlab = tf.Variable(np.zeros((batch_size,num_labels)), dtype=tf.float32)\n",
    "        self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32)\n",
    "\n",
    "        # and here's what we use to assign them\n",
    "        self.assign_timg = tf.placeholder(tf.float32, shape)\n",
    "        self.assign_tlab = tf.placeholder(tf.float32, (batch_size,num_labels))\n",
    "        self.assign_const = tf.placeholder(tf.float32, [batch_size])\n",
    "        \n",
    "        # the resulting image, tanh'd to keep bounded from boxmin to boxmax\n",
    "        self.boxmul = (boxmax - boxmin) / 2.\n",
    "        self.boxplus = (boxmin + boxmax) / 2.\n",
    "        self.newimg = tf.tanh(modifier + self.timg) * self.boxmul + self.boxplus\n",
    "        \n",
    "        # prediction BEFORE-SOFTMAX of the model\n",
    "        self.output = model.predict(self.newimg)\n",
    "        print(self.newimg.shape)\n",
    "        print(self.output.shape)\n",
    "        print(type(self.output))\n",
    "        # distance to the input data\n",
    "        self.l2dist = tf.reduce_sum(tf.square(self.newimg-(tf.tanh(self.timg) * self.boxmul + self.boxplus)),[1,2,3])\n",
    "        \n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        real = tf.reduce_sum((self.tlab)*self.output,1)\n",
    "        other = tf.reduce_max((1-self.tlab)*self.output - (self.tlab*10000),1)\n",
    "\n",
    "        if self.TARGETED:\n",
    "            # if targetted, optimize for making the other class most likely\n",
    "            loss1 = tf.maximum(0.0, other-real+self.CONFIDENCE)\n",
    "        else:\n",
    "            # if untargeted, optimize for making this class least likely.\n",
    "            loss1 = tf.maximum(0.0, real-other+self.CONFIDENCE)\n",
    "\n",
    "        # sum up the losses\n",
    "        self.loss2 = tf.reduce_sum(self.l2dist)\n",
    "        self.loss1 = tf.reduce_sum(self.const*loss1)\n",
    "        self.loss = self.loss1+self.loss2\n",
    "        \n",
    "        # Setup the adam optimizer and keep track of variables we're creating\n",
    "        start_vars = set(x.name for x in tf.global_variables())\n",
    "        optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "        self.train = optimizer.minimize(self.loss, var_list=[modifier])\n",
    "        end_vars = tf.global_variables()\n",
    "        new_vars = [x for x in end_vars if x.name not in start_vars]\n",
    "        \n",
    "        # these are the variables to initialize when we run\n",
    "        self.setup = []\n",
    "        self.setup.append(self.timg.assign(self.assign_timg))\n",
    "        self.setup.append(self.tlab.assign(self.assign_tlab))\n",
    "        self.setup.append(self.const.assign(self.assign_const))\n",
    "        \n",
    "        self.init = tf.variables_initializer(var_list=[modifier]+new_vars)\n",
    "\n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the L_2 attack on the given images for the given targets.\n",
    "\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        print('go up to',len(imgs))\n",
    "        for i in range(0,len(imgs),self.batch_size):\n",
    "            print('tick',i)\n",
    "            r.extend(self.attack_batch(imgs[i:i+self.batch_size], targets[i:i+self.batch_size]))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_batch(self, imgs, labs):\n",
    "        \"\"\"\n",
    "        Run the attack on a batch of images and labels.\n",
    "        \"\"\"\n",
    "        def compare(x,y):\n",
    "            if not isinstance(x, (float, int, np.int64)):\n",
    "                x = np.copy(x)\n",
    "                if self.TARGETED:\n",
    "                    x[y] -= self.CONFIDENCE\n",
    "                else:\n",
    "                    x[y] += self.CONFIDENCE\n",
    "                x = np.argmax(x)\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # convert to tanh-space\n",
    "        imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n",
    "\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        CONST = np.ones(batch_size)*self.initial_const\n",
    "        upper_bound = np.ones(batch_size)*1e10\n",
    "\n",
    "        # the best l2, score, and image attack\n",
    "        o_bestl2 = [1e10]*batch_size\n",
    "        o_bestscore = [-1]*batch_size\n",
    "        o_bestattack = [np.zeros(imgs[0].shape)]*batch_size\n",
    "        \n",
    "        for outer_step in range(self.BINARY_SEARCH_STEPS):\n",
    "            print(o_bestl2)\n",
    "            # completely reset adam's internal state.\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "            #Take in the current batch.\n",
    "            batch = imgs[:batch_size]\n",
    "            batchlab = labs[:batch_size]\n",
    "    \n",
    "            bestl2 = [1e10]*batch_size\n",
    "            bestscore = [-1]*batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS-1:\n",
    "                CONST = upper_bound\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            self.sess.run(self.setup, {self.assign_timg: batch,\n",
    "                                       self.assign_tlab: batchlab,\n",
    "                                       self.assign_const: CONST})\n",
    "            \n",
    "            prev = 1e6\n",
    "            for iteration in range(self.MAX_ITERATIONS):\n",
    "                # perform the attack \n",
    "                _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss,\n",
    "                                                         self.l2dist, self.output,\n",
    "                                                         self.newimg])\n",
    "\n",
    "                # print out the losses every 10%\n",
    "                if iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    print(iteration,self.sess.run((self.loss,self.loss1,self.loss2)))\n",
    "\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if self.ABORT_EARLY and iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    if l > prev*.9999:\n",
    "                        break\n",
    "                    prev = l\n",
    "\n",
    "                # adjust the best result found so far\n",
    "                for e,(l2,sc,ii) in enumerate(zip(l2s,scores,nimg)):\n",
    "                    if l2 < bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = np.argmax(sc)\n",
    "                    if l2 < o_bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = np.argmax(sc)\n",
    "                        o_bestattack[e] = ii\n",
    "\n",
    "            # adjust the constant as needed\n",
    "            for e in range(batch_size):\n",
    "                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    #          or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "        # return the best solution found\n",
    "        o_bestl2 = np.array(o_bestl2)\n",
    "        return o_bestattack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"1_2\"></a>\n",
    "\n",
    "## 1.2  Testing C&W\n",
    "\n",
    "We launch the C&W attack against a MNIST classifier in this section. We reuse `mnist_cnn_model.h5` as the target classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2.  Adversarial training as a defence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"2_1\"></a>\n",
    "\n",
    "## 2.1  Implementing adversarial training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"2_2\"></a>\n",
    "\n",
    "## 2.2  Black-box C&W attacks against adversarial training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"2_3\"></a>\n",
    "\n",
    "## 2.3  White-box C&W attacks against adversarial training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <div  style=\"text-align:center\">**THE END**</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
