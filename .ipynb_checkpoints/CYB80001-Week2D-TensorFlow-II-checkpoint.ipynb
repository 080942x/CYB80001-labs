{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMWNCaNid4Ls"
   },
   "source": [
    "# CYB80001 System Security Project\n",
    "Prepared by **Derui (Derek) Wang**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTHctvJed4Lt"
   },
   "source": [
    "# Session 2D - Deep Feedforward Neural Networks\n",
    "\n",
    "**The purpose of this session is to demonstrate how to use TensorFlow to develop machine learning algorithms and deep neural network models. In this practical session, we present the following topics:**\n",
    "\n",
    "1. How to implement classification algorithms using Tensorflow\n",
    "2. Learning  standard and advanced gradient optimization methods and use these predefined optimizer inTensorFlow\n",
    "3. How to build a deep neural networks for image classification problems using TensorFlow\n",
    "\n",
    "** References and additional reading and resources**\n",
    "- [An Introduction to Implementing Neural Networks using TensorFlow](https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/)\n",
    "- [Tensorflow Tutorials](https://www.tensorflow.org/tutorials/)\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/)\n",
    "- [37 Reasons why your Neural Network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "\n",
    "\n",
    "### Part 1 Classification with TensorFlow\n",
    "\n",
    "1.1 [Binary classification](#1_1)\n",
    "\n",
    "1.2 [Multiclass classification](#1_2)\n",
    "\n",
    "1.3 [Gradient learning](#1_3)\n",
    "\n",
    "1.4 [Putting it altogether for multiclass classification](#1_4)\n",
    "\n",
    "1.5 [Exercises](#1_5)\n",
    "\n",
    "\n",
    "### Part 2 Deep Feedforward Neural Networks\n",
    "\n",
    "2.1 [Construction phase](#2_1)\n",
    "\n",
    "2.2 [Execution phase](#2_2)\n",
    "\n",
    "2.3 [Combining two phases using API functions of TensorFlow](#2_3)\n",
    "\n",
    "2.4 [Exercises](#2_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pbqheIRd4Lt"
   },
   "source": [
    "# Part 1. Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qe3xalS1d4Lu"
   },
   "source": [
    "For a classification problem, we wish to predict disrcrete label $y$ given features $\\mathbf{x}=\\left[x_{1},x_{2},...,x_{N}\\right]^{T}$, wherein $y\\in\\left\\{0,1\\right\\}$ for binary classification and $y\\in\\left\\{ 1,...,K\\right\\}$ for multiclass classification. We typically solve this problem by learning a function to predict log-probability that an example belong to each class and then apply the principle of maximum likelihood to derive the loss function. Let's first consider the binary case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwHPBnNid4Lv"
   },
   "source": [
    "<a id = \"1_1\"></a>\n",
    "\n",
    "## 1.1. Binary classification\n",
    "#### Problem formulation\n",
    "For the binary case, we need to learn the class-conditional densities:  \n",
    "\n",
    "$$\n",
    "p(y=1 \\mid \\mathbf{x})\t=\t\\frac{p(\\mathbf{x} \\mid y=1)p(y=1)}{p(\\mathbf{x} \\mid y=0)p(y=0)+p(\\mathbf{x} \\mid y=1)p(y=1)}\n",
    "\t=\t\\frac{1}{1+\\frac{p(\\mathbf{x} \\mid y=0)p(y=0)}{p(\\mathbf{x} \\mid y=1)p(y=1)}}\n",
    "\t=\t\\frac{1}{1+\\exp(-a)}\n",
    "\t=\t\\sigma(a)\n",
    "$$\n",
    "\n",
    "where $a=\\log\\frac{p(\\mathbf{x} \\mid y=1)p(y=1)}{p(\\mathbf{x} \\mid y=0)p(y=0)}$ (also known as log odds), and $\\sigma(a)$ is the **logistic sigmoid function** defined by:  \n",
    "\n",
    "$$\n",
    "\\sigma(a)=\\frac{1}{1+\\exp(-a)}\n",
    "$$ \n",
    "\n",
    "If we use a function $f$, which can be modeled using simple linear regression or a neural network with parameter $\\mathbf{\\theta}$, to estimate $a$, we'll have:  \n",
    "\n",
    "$$\n",
    "p(y=1 \\mid \\mathbf{x})=\\sigma(a)=\\sigma(f_{\\mathbf{\\theta}}(\\mathbf{x}))\n",
    "$$ \n",
    "\n",
    "For a set of $M$ training examples with binary labels $\\left\\{(\\mathbf{x}^{(i)},y^{(i)}):i=1,...,M\\right\\}$, the probability according to our model will be:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{M}p(y=y^{(i)} \\mid \\mathbf{x}^{(i)})=\\prod_{i=1}^{M}\\sigma(a_{i})^{y^{(i)}}(1-\\sigma(a_{i}))^{(1-y^{(i)})}\n",
    "$$ \n",
    "\n",
    "wherein $a_{i}=f_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lWxC1Qcd4Lv"
   },
   "source": [
    "#### <span style=\"color:#0b486b\">Loss function:</span>\n",
    "\n",
    "We need to find $\\mathbf{\\theta}$ that maximizes the likelihood $\\mathcal{L}$. This can be done more easily by minimizing the negative log-likelihood. Therefore, we have the cost function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{\\theta})=\\sum_{i=1}^{M}-y^{(i)}\\log\\sigma(a_{i})-(1-y^{(i)})\\log(1-\\sigma(a_{i}))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E43juoyNd4Lw"
   },
   "source": [
    "#### Implementation with TensorFlow\n",
    "\n",
    "In terms of implementation, given a scalar $a$ in the case of logistic classification , we can use the following code to calculate the conditional probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3YWALrTFd4Lx",
    "outputId": "e5c096cd-a5fd-40c1-9b97-f4146dd88cf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewe\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999546  0.11920292]\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "a = tf.constant([10.0, -2.0])\n",
    "y_proba = tf.nn.sigmoid(a)\n",
    "with tf.Session() as sess:\n",
    "    print(y_proba.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-TZX9OQd4L0"
   },
   "source": [
    "Assuming that we know the true labels, then we can calculate the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DQgJR2ROd4L1",
    "outputId": "affc7072-2ac8-4d31-9302-d19f6feb271c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06348671\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([1.0, 0.0])\n",
    "loss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nxc0swZVd4L3"
   },
   "source": [
    "However, calculating the probability is numerically unstable when $a$ is too large or too small due to the exponential operation. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vBGHjB_Gd4L3",
    "outputId": "ba13c9fc-0cd5-4593-bd65-2b8e036c081f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.00669285]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([20.0, -5.0])\n",
    "y_proba = tf.nn.sigmoid(a)\n",
    "y = tf.constant([1.0, 0.0])\n",
    "loss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\n",
    "with tf.Session() as sess:\n",
    "    print(y_proba.eval())\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUOOFYLUd4L5"
   },
   "source": [
    "We are thus recommended to estimate the loss directly from logits, using a function from TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1Bo4h6g0d4L6",
    "outputId": "59a21e68-280f-4175-cade-2c6a0a5932dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033576752\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=a))\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8iDOn5qd4L8"
   },
   "source": [
    "<a id = \"1_2\"></a>\n",
    "\n",
    "## 1.2. Multiclass classification\n",
    "\n",
    "In multiclass classification problem when $K>2$, we have:\n",
    "\n",
    "$$\n",
    "p(y=k \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid y=k)p(y=k)}{\\sum_{j=1}^{K}p(\\mathbf{x} \\mid y=j)p(y=j)}\n",
    "\t= \\frac{\\exp(a_{k})}{\\sum_{j=1}^{K}\\exp(a_{j})}\n",
    "\t= \\sigma(\\mathbf{a})_{k}\n",
    "$$ \n",
    "\n",
    "where $a_k=\\log p(\\mathbf{x} \\mid y=k)p(y=k)$ is called logits by TensorFlow (This is different from the logit function in statistics), and $\\sigma(\\mathbf{a})_{k}$ is the softmax function defined by:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{a})_{k}=\\frac{\\exp(a_{k})}{\\sum_{j=1}^{K}\\exp(a_{j})}\n",
    "$$\n",
    "\n",
    "Similary to the binary classification, we can use simple linear regression or a neural network with parameter $\\mathbf{\\theta}$   to model a function $f_{\\mathbf{\\theta}}(\\mathbf{x})=\\left[a_{1},a_{2},...,a_{K}\\right]^{T}$. For a set of $M$ training examples with multinominal labels $\\left\\{(\\mathbf{x}^{(i)},y^{(i)}):i=1,...,M\\right\\}$, the probability according to our model will be:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{M}p(y=y^{(i)}|\\mathbf{x}^{(i)})=\\prod_{i=1}^{M}\\sigma(\\mathbf{a}^{(i)})_{y^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ3MYHyUd4L9"
   },
   "source": [
    "#### <span style=\"color:#0b486b\">Loss function:</span>\n",
    "\n",
    "We need to find $\\mathbf{\\theta}$ that minimizes the negative log-likelihood. So, we have the loss function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{\\theta})=-\\sum_{i=1}^{M}\\log\\sigma(\\mathbf{a}^{(i)})_{y^{(i)}}\n",
    "$$\n",
    "If we express the label for the $i$-th example as a **one-hot** vector $\\mathbf{y}^{(i)}=\\left[y_{1}^{(i)},y_{2}^{(i)},...,y_{K}^{(i)}\\right]^{T}$, where $y_{k}^{(i)}=1$ if the example belongs to class $k$, and $y_{k}^{(i)}=0$ otherwise, the cost function can be rewritten as:   \n",
    "\n",
    "$$J(\\mathbf{\\theta})=-\\sum_{i=1}^{M}\\sum_{j=1}^{K}y_{j}^{(i)}\\log \\sigma(\\mathbf{a}^{(i)})_{j}$$\n",
    "\n",
    "We now can use the following code to calculate the conditional probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "T5ODTELid4L-",
    "outputId": "21f2ebc4-d4fb-4878-f1c8-a67098762cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999380e-01 6.1441742e-06]\n",
      " [3.3535014e-04 9.9966466e-01]\n",
      " [9.9995458e-01 4.5397868e-05]]\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "a = tf.constant([[10.0, -2.0], [-5.0, 3.0], [30, 20]])\n",
    "y_proba = tf.nn.softmax(a)\n",
    "with tf.Session() as sess:\n",
    "    print(y_proba.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwaEuFfud4MA"
   },
   "source": [
    "Then we can calculate the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CTLrEE8Wd4MB",
    "outputId": "3850b590-1b98-4233-d452-e5ee9ea10128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012900354\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([[1.0, 0], [0, 1], [1, 0]])\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_proba), axis=1))\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2N-Ulandd4ME"
   },
   "source": [
    "However, similar to the problem in binary classification, calculating the probability is numerically unstable when $a$ is too large or too small due to the exponential operation. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IxtaJTWqd4MF",
    "outputId": "38eacbe5-aa3f-40b4-dbfc-9d6c432a2aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 1.3887944e-11]\n",
      " [3.3535014e-04 9.9966466e-01]]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[20.0, -5.0], [-5.0, 3.0]])\n",
    "y_proba = tf.nn.softmax(a)\n",
    "with tf.Session() as sess:\n",
    "    print(y_proba.eval())\n",
    "y = tf.constant([[1.0, 0.0], [0.0, 1.0]])\n",
    "loss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScLhS3jCd4MH"
   },
   "source": [
    "We are thus recommended to estimate the loss directly from logits, using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OAZMFjQtd4MH",
    "outputId": "1e39e3cd-f9af-4e7b-cc92-1df86a04026b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016769934\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=a)) \n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9KMlCu3d4MJ"
   },
   "source": [
    "When labels are encoded in a one dimensional vector (instead of a one-hot vector), we use the sparse version of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NQBSkIWpd4MK",
    "outputId": "773eca05-0b8f-43e8-ea35-6f44c49ddf96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016769934\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[20.0, -5.0], [-5.0, 3.0]])\n",
    "y = tf.constant([0, 1])\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=a))\n",
    "with tf.Session() as sess:\n",
    "    print(loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv6R8cFnd4MM"
   },
   "source": [
    "<a id = \"1_3\"></a>\n",
    "\n",
    "## 1.3. Gradient learning\n",
    "\n",
    "<img src=\"imgs/GD.jpg\">\n",
    "\n",
    "Once you define the loss function, you can minimize the loss by using gradient descent algorithm or its variants. The idea is that starting at a point in the parameter space, following the directions of gradients estimated at that point will increase the loss function. To reduce the loss, you should go the opposite directions. So, the basic gradient descent algorithm is to iteratively update the parameters until reaching a global (local) minima. For parameters $\\mathbf{\\theta}$ with gradients $\\frac{\\partial{J}}{\\partial{\\mathbf{\\theta}}}$, the update will take the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}=\\mathbf{\\theta}-\\eta\\frac{\\partial{J}}{\\partial{\\mathbf{\\theta}}}\n",
    "$$\n",
    "\n",
    "where $\\eta > 0$ is the learning rate. Learning rate $\\eta$ is an important parameter to be tuned. A low learning rate cause the model to learn slowly. High learning rate can help the model learn faster but may fail to converge as the parameters will bound around the convergence point in the parameter space.\n",
    "\n",
    "A simple way to select learning rate is to try different learning rates, typically between `1e-5` and `1`, and draw the graph of loss after each iterations or epochs. The following image sketches some scenerios of good or bad learning rate:\n",
    "\n",
    "<img src=\"imgs/LR.png\">\n",
    "\n",
    "There are variants of gradient descent that can reach convergence faster but they are all based on gradients. In what follows, we'll briefly talk about some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ih0UpT1vd4MN"
   },
   "source": [
    "### 1.3.1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "For gradient descent, we need to empirically estimate the average loss using all training data:\n",
    "$$\n",
    "J(\\mathbf{\\theta})=\\frac{1}{M}\\sum_{i=1}^{M}\\mathcal{L}(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}$ is the per-example loss $\\mathcal{L}(\\mathbf{x}, y, \\mathbf{\\theta})=-\\log{p(y|\\mathbf{x};\\mathbf{\\theta})}$.\n",
    "Unfortunately, estimating the average loss over a large training set is computationally expensive. Stochastic gradient descent (**SGD**) is an extension of the gradient descent algorithm. For each step of the training algorithm, we can sample a minibatch of example $\\mathbb{B}=\\{\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},..,\\mathbf{x}^{(m^{\\prime})}\\}$ The minibatch size $m^{\\prime}$ is typically chosen to be a relatively small number of examples, ranging from 1 to a few hundred. The estimate of the gradient will be:\n",
    "$$\n",
    "\\mathbf{g}=\\frac{1}{m^{\\prime}}\\nabla_{\\mathbf{\\theta}}\\sum_{i=1}^{m^{\\prime}}\\mathcal{L}(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "The weight update in stochastic gradient descent will be:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}=\\mathbf{\\theta}-\\eta\\mathbf{g}\n",
    "$$\n",
    "\n",
    "Optimization algorithms that use only a single example at a time are called stochastic methods while optimization algorithms that use more than a single examples at a time are traditionally called minibatch stochastic methods. It's now common to simply call them stochastic methods.\n",
    "\n",
    "**Some notes**:\n",
    "- Using small batch size can lead to unstable estimates. The standard error of the mean estimated from $m^{\\prime}$ samples is given by $\\sigma\\mathbin{/}\\sqrt{m^{\\prime}}$. So, Training with such a small batch size might require a small learning rate to maintain stability due to the high variance in the estimate of the gradient.\n",
    "- Large batches provide a more accurate estimate of the gradient. However, as we increase batch size from 100 to 10,000, the latter requires 100 times more computation than the former but reduces the standard error of the mean only by a factor of 10.\n",
    "- Small batches can offer regularizing effect perhaps due to the noise they add to during the learning process. Generalization error is often best for a batch size of 1. However, the total runtime can be very high due to the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.\n",
    "- Multicore architectures are usually underutilized by extremely small batches.\n",
    "- Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models.\n",
    "- The simplest way to do feed a minibatch to our computational graph in Tensorflow is to use placeholder nodes. They are typically used to pass the training data to TensorFlow during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3HdlkL6d4MN"
   },
   "source": [
    "### 1.3.2. Momentum\n",
    "\n",
    "Stochastic gradient descent can sometimes be slow, especially when the error surface has a large curvature. Imagine the loss as the height of a canyon with steep sides. Randomly initializing the parameters is like starting at a location in the canyon and optimizing the loss is like going down the canyon.\n",
    "\n",
    "Momentum algorithm, proposed by Boris Polyak in 1964, accelerates learning in such situation by accumulating an exponentially decaying moving average of past gradients and continues to move in their direction. Momentum introduces a velocity vector $\\mathbf{v}$. It is the direction and the speed at which the parameters move through the hyperparameter space. The velocity is set to an exponentially decaying average of the negative gradient. We can interpret the decay as a result of friction that keeps the velocity from growing too large. The name momentum is derived from a physic analogy, in which the negative gradient is a force moving a particle through parameter space, according to Newton’s laws of motion. Momentum in physics is mass times velocity. In the momentum learning algorithm, we assume unit mass, so the velocity vector $\\mathbf{v}$ may also be regarded as the momentum $\\mathbf{m}$ of the particle.\n",
    "\n",
    "At each iteration, the local gradient (multiplied by the learning rate $\\eta$) is added to the momentum vector $\\mathbf{m}$, and you can update the weights by simply subtract this momentum vector:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m} &= \\beta\\mathbf{m} + \\eta\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\\\\n",
    "\\mathbf{\\theta} &= \\mathbf{\\theta} - \\mathbf{m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\beta$ is the momentum hyperparameter. The higher the hyperparameter is, the bigger the influence of past gradient is. The momentum hyperparameter is usually start at a small number, such as 0.5, and increased gradually to 0.9 or 0.99.\n",
    "\n",
    "When local gradient keeps pointing to a certain direction, momentum will pickup in that direction, allowing the update path to tranverse the canyon lengthwise and converge faster, as shown by the red path in the picture above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9ikpgnGd4MO"
   },
   "source": [
    "### 1.3.3. AdaGrad\n",
    "\n",
    "Consider a elongated bowl with a gentle slope, gradient descent starts by starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. We may wish a greater progress in the more gently slopped directions of the hyperparmeter space.\n",
    "\n",
    "AdaGrad achieves this goal by individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all of their historical squared values. The parameters with the largest partial derivative of the loss have a correspondingly rapid decrease in their learning rate, while parameters with small partial derivatives have a relatively small decrease in their learning rate.\n",
    "\n",
    "Each update in the AdaGrad algorithm takes two steps. The first step in AdaGrad algorithm is to accumulate the square of the gradients into the vector $\\mathbf{s}$. The second step is to update parameters as usual, however with one big difference: the learning rate is scaled down by a factor of $\\sqrt{\\mathbf{s}+\\epsilon}$ where $\\epsilon$ is a small number, typically 1e-8, to avoid division by zero.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{s} &= \\mathbf{s}+\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n",
    "\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\nabla_{\\theta}J(\\mathbf{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\otimes$ is element-wise product and $\\oslash$ is element-wise division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-be4lI4d4MO"
   },
   "source": [
    "### 1.3.4. RMSProp\n",
    "\n",
    "Empirically AdaGrad has been found that—for training deep neural network models—the accumulation of squared gradients from the beginning of training can result in a premature and excessive decrease in the effective learning rate. AdaGrad performs well for some but not all deep learning models. AdaGrad is designed to converge rapidly when applied to a convex function. When applied to a non-convex function to train a neural network, the learning trajectory may pass through many different structures and eventually arrive at a region that is a locally convex bowl AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure.\n",
    "\n",
    "The RMSProp algorithm modifies AdaGrad by changing the gradient accumulation in the first step into an exponentially weighted moving average:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{s} &= \\beta\\mathbf{s}+(1-\\beta)\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n",
    "\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\nabla_{\\theta}J(\\mathbf{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using exponentially decaying average allows RMSProp to discard history from the extreme past so that it can converge rapidly after finding a convex bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jaVC0sHod4MP"
   },
   "source": [
    "### 1.3.5. Adam\n",
    "\n",
    "**Adam** combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared\n",
    "gradients.\n",
    "\n",
    "There are five steps in Adam:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m} &= \\beta_{1}\\mathbf{m}+(1-\\beta_{1})\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\\\\n",
    "\\mathbf{s} &= \\beta_{2}\\mathbf{s}+(1-\\beta_{2})\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n",
    "\\mathbf{m} &= \\frac{\\mathbf{m}}{1-\\beta_{1}^{t}} \\\\\n",
    "\\mathbf{s} &= \\frac{\\mathbf{s}}{1-\\beta_{2}^{t}} \\\\\n",
    "\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\mathbf{m}\\oslash\\sqrt{\\mathbf{s}+\\epsilon} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t$ is the iterationi number, starting at 1.\n",
    "\n",
    "Step 1 and 2 estimate the first and second order moment of the gradient. Steps 3 and 4 are somewhat of a technical detail: since $m$ and $s$ are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost $m$ and $s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_{1}$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_{2}$ is often initialized to 0.999. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{-8}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZJSm94dd4MQ"
   },
   "source": [
    "### 1.3.6. Implementation in TensorFlow\n",
    "\n",
    "To implement gradient descent-based optimization, you'll need to calculate the gradients of the loss. You can manually derive the gradients from the cost function. In the case of Linear Regression, it is reasonably easy, but if you had to do this with deep neural networks we would get quite a headache: it would be tedious and error-prone. You can instead use TensorFlow’s autodiff feature to let TensorFlow compute the gradients automatically or use a couple of TensorFlow’s out-of-the-box optimizers.\n",
    "\n",
    "When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be much slower. You can do this using TensorFlow, NumPy, Scikit-Learn’s StandardScaler, or any other solution you prefer. The\n",
    "following code assumes that this normalization has already been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aU7QTIXOd4MQ"
   },
   "source": [
    "TensorFlow also provides a number of **off-the-shelf optimizers**, including a Gradient Descent optimizer. You can simply declare an optimizer and add an *`op`* that performs an upgrade step:\n",
    "\n",
    "``optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)``\n",
    "\n",
    "``training_op = optimizer.minimize(mse)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlshZvjxd4MR"
   },
   "source": [
    "Current list of optimizers implemented in TensorFlow is:\n",
    "1. tf.train.GradientDescentOptimizer\n",
    "* tf.train.AdadeltaOptimizer\n",
    "* tf.train.AdagradOptimizer\n",
    "* tf.train.AdagradDAOptimizer\n",
    "* tf.train.MomentumOptimizer\n",
    "* tf.train.AdamOptimizer\n",
    "* tf.train.FtrlOptimizer\n",
    "* tf.train.ProximalGradientDescentOptimizer\n",
    "* tf.train.ProximalAdagradOptimizer\n",
    "* tf.train.RMSPropOptimizer\n",
    "\n",
    "... and more are coming. ***Reference***: [https://www.tensorflow.org/api_guides/python/train](https://www.tensorflow.org/api_guides/python/train)\n",
    "\n",
    "If you want to try momentum optimizer or Adam optimizer, just replace with one of the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4OAvum86d4MR"
   },
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mwvkk4R8d4MT"
   },
   "source": [
    "<a id = \"1_4\"></a>\n",
    "\n",
    "## 1.4. Putting it altogether for multiclass classification\n",
    "\n",
    "Now, we've had all ingredients to build a logistic or softmax classification. Let's build a simple softmax classification with MNIST dataset.\n",
    "\n",
    "* #### <span style=\"color:#0b486b\">Step 1: Load or download the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfiKGYLHd4MU",
    "outputId": "0bb700e7-8dda-4c77-f730-0b4c310c5139"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weilu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/\")\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xs53KH8dd4MW"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2: Build the graph using TensorFlow</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kS-VWewVd4MW"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n",
    "y = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\n",
    "b = tf.Variable(tf.zeros([n_outputs]), name='biases')\n",
    "\n",
    "logits = tf.add(tf.matmul(X, W), b, name='logits')\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    grad_W, grad_b = tf.gradients(loss, [W, b])\n",
    "    update_W = tf.assign(W, W - learning_rate * grad_W)\n",
    "    update_b = tf.assign(b, b - learning_rate * grad_b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJ0fzsDSd4MZ"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3: Train the model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9f5bAZ66d4Ma"
   },
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([update_W, update_b], feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oI3rgEsBd4Mb"
   },
   "source": [
    "We end up with the testing acurracy of about 92% that is not bad result. Now let's try using black-box gradient descent optimizer of TensorFlow. We will change the code in **Step 2**, which we denote as **Step 2(a)**, **Step 2(b)**,... The code for **Step 3** remains the same the we call **Step 3 (replicate)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wA759iJd4Mb"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2(a): Build the graph using TensorFlow gradient descent optimizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gs-dIrHcd4Md"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n",
    "y = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\n",
    "b = tf.Variable(tf.zeros([n_outputs]), name='biases')\n",
    "\n",
    "logits = tf.add(tf.matmul(X, W), b, name='logits')\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhkdC8Nsd4Me"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3 (replicate): Train the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iQr3tsFQd4Mf",
    "outputId": "f84f4690-0d8a-497e-e5d4-99d525614e8c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain accuracy\tTest accuracy\n",
      "0\t0.740000\t0.872800\n",
      "1\t0.800000\t0.886000\n",
      "2\t0.860000\t0.894800\n",
      "3\t0.900000\t0.898900\n",
      "4\t0.900000\t0.902200\n",
      "5\t0.900000\t0.904700\n",
      "6\t0.900000\t0.909000\n",
      "7\t0.920000\t0.908600\n",
      "8\t0.920000\t0.911400\n",
      "9\t0.940000\t0.911300\n",
      "10\t0.860000\t0.912700\n",
      "11\t0.980000\t0.913100\n",
      "12\t0.880000\t0.914500\n",
      "13\t0.880000\t0.915100\n",
      "14\t0.920000\t0.916100\n",
      "15\t0.940000\t0.914800\n",
      "16\t0.860000\t0.916300\n",
      "17\t0.920000\t0.917100\n",
      "18\t0.980000\t0.917200\n",
      "19\t1.000000\t0.918000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1nQWmMWd4Mj"
   },
   "source": [
    "We end up with the same result as of using auto-gradient, which is expected. Now let's try **Adam optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQQIXDbDd4Mk"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2(b): Build the graph using TensorFlow Adam optimizer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XwfRjSbKd4Ml"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\n",
    "y = tf.placeholder(tf.int64, shape=[None], name='y')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\n",
    "b = tf.Variable(tf.zeros([n_outputs]), name='biases')\n",
    "\n",
    "logits = tf.add(tf.matmul(X, W), b, name='logits')\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8TVsBKhd4Mm"
   },
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3 (replicate): Train the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "p40IM_qwd4Mn",
    "outputId": "fc2a7414-a4e1-4180-940c-2650ef771352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain accuracy\tTest accuracy\n",
      "0\t0.920000\t0.911300\n",
      "1\t0.920000\t0.919700\n",
      "2\t0.940000\t0.920900\n",
      "3\t0.920000\t0.923400\n",
      "4\t0.900000\t0.925400\n",
      "5\t0.960000\t0.926300\n",
      "6\t0.920000\t0.927100\n",
      "7\t0.920000\t0.927800\n",
      "8\t0.900000\t0.927300\n",
      "9\t0.900000\t0.926300\n",
      "10\t0.860000\t0.928100\n",
      "11\t0.940000\t0.925800\n",
      "12\t0.940000\t0.925400\n",
      "13\t0.940000\t0.927300\n",
      "14\t0.920000\t0.927300\n",
      "15\t0.940000\t0.928100\n",
      "16\t0.960000\t0.927000\n",
      "17\t0.960000\t0.925000\n",
      "18\t0.980000\t0.927000\n",
      "19\t0.960000\t0.928400\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyylnAdId4Mo"
   },
   "source": [
    "We have just improved the accuracy by 1%, which is not really cool but promising.\n",
    "\n",
    "Now we are going to use deeper and more powerful models in the consequence section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fiv9hFOd4Mp"
   },
   "source": [
    "<a id = \"1_5\"></a>\n",
    "\n",
    "## 1.5  Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JGki4LZd4Mp"
   },
   "source": [
    "Train the classifier in Section 1.3 the using some other optimizers provided by TensorFlow: [Adadelta](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). You can try with different parameters of these optimizers and report the best values (of parameters and corresponding performances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1mqjxt5d4Mq"
   },
   "source": [
    "# Part 2. Deep Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkdlFw0Wd4Mq"
   },
   "source": [
    "Deep feedforward neural network (DNN) basically is an advanced version of the multilayer perceptron (MLP) with nonlinear hidden activations. DNNs are at the very core of *Deep Learning*. They are versatile, powerful, and scalable, making them\n",
    "ideal for tackling large and highly complex Machine Learning tasks, such as classifying billions of images\n",
    "(e.g., Google Images), powering speech recognition services (e.g., Apple’s Siri), recommending the best\n",
    "videos to watch to hundreds of millions of users everyday (e.g., YouTube), or learning to beat the world\n",
    "champion at the game of Go by examining millions of past games and then playing against itself\n",
    "(DeepMind’s AlphaGo).<br>\n",
    "\n",
    "In this session, we are going to use TensorFlow's Python API to implement MNIST digit classification problem. We will use minibatch gradient descent to train our network. Generally, the first step is the construction phase, i.e., building the TensorFlow graph, and the second step is the execution phase, where you actually run the graph to train the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxo1YR4gd4Mr"
   },
   "source": [
    "Now let's first import the TensorFlow library and load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mioezRrNd4Mr",
    "outputId": "91d0b0ad-f5b8-4d4f-8368-168959be4aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/\")\n",
    "x_train = mnist.train.images\n",
    "x_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dssqy_W0d4Mt"
   },
   "source": [
    "When developing an application using an DNN, we usually following two main stages:\n",
    "\n",
    "1. Construction phase\n",
    "2. Execution phase (training and testing)\n",
    "3. Regularization\n",
    "\n",
    "However, DNNs are easy to be overfitting and producing low performances on testing datasets. Regularization techniques are often used in DNNs to prevent them from overfitting. We also introduce some regularization methods which are popularly used in DNNs  then.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-724PuTDd4Mu"
   },
   "source": [
    "<a id = \"2_1\"></a>\n",
    "\n",
    "## 2.1. Construction phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEiErusid4Mu"
   },
   "source": [
    "Let’s start. First, we need to specify the number of inputs and outputs, and set the number of hidden neurons in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYCgSh7vd4Mv"
   },
   "outputs": [],
   "source": [
    "num_inputs = 28 * 28 # this is the size of images in pixels \n",
    "num_hidden1 = 300\n",
    "num_hidden2 = 100\n",
    "num_outputs = 10  # this is the number of classes (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmOiI-Gyd4Mx"
   },
   "source": [
    "Next we use placeholder nodes to represent the training data and labels. The shapes of *`x`* and *`y`* are only partially defined to be able to take an arbitrary minibatch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSWH3WQMd4Mx"
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, num_inputs], name=\"x\")\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8d7exPOwd4Mz"
   },
   "source": [
    "Now we define a function, i.e., layer creator, that can help to create a layer in our multiple layer network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_ht7lSJd4Mz"
   },
   "outputs": [],
   "source": [
    "def neuron_layer(x, num_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        num_inputs = int(x.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(num_inputs)\n",
    "        init = tf.truncated_normal([num_inputs, num_neurons], stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([num_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(x, W) + b\n",
    "    if activation == \"relu\":\n",
    "        return tf.nn.relu(z)\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apEZw8Ivd4M1"
   },
   "source": [
    "Let's go through the above code line by line:\n",
    "\n",
    "1. First we create a name scope using the name of the layer: it will contain all the computation nodes for this neuron layer. This is optional, but the graph will look much nicer in TensorBoard if its nodes are well organized.\n",
    "2. Next, we get the number of inputs by looking up the input matrix’s shape and getting the size of the second dimension (the first dimension is for instances).\n",
    "3. The next three lines create a W variable that will hold the weights matrix. It will be a 2D tensor containing all the connection weights between each input and each neuron; hence, its shape will be `(n_inputs, n_neurons)`. It will be initialized randomly, using a [truncated normal (Gaussian) distribution](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) with a standard deviation of $\\frac{2}{\\sqrt{n_{inputs}}}$. Using this specific standard deviation helps the algorithm converge much faster.\n",
    "4. The next line creates $\\mathbf{b}$ variable for biases, initialized to `0` (no symmetry issue in this case), with one bias parameter per neuron.\n",
    "5. Then we create a subgraph to compute $\\mathbf{Z} = \\mathbf{W}^{T} \\mathbf{X}  + \\mathbf{b}$. This vectorized implementation will efficiently compute the weighted sums of the inputs plus the bias term for each and every neuron in the layer, for all the instances in the batch in just one shot.\n",
    "6. Finally, if the activation parameter is set to \"relu\", the code returns *`relu(z)`* (i.e., *`max(0,z)`*), or else it just returns a linear *`z`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUEYkPJcd4M2"
   },
   "source": [
    "Now let’s use the *`neuron_layer`* function to create the deep neural network! The first hidden layer takes *`x`* as its input. The second takes the output of the first hidden layer as its input. And finally, the output layer takes the output of the second hidden layer as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nI9z2M_ld4M3"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(x, num_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, num_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, num_outputs, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jo4Ecz0xd4M5"
   },
   "source": [
    "We need to define a loss function. As discussed in the last lecture, it's more numerical to estimate the cross entropy loss directly from logits using TensorFlow function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Wmuz3HKd4M5"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\\\n",
    "                                        logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "papmdV_nd4M6"
   },
   "source": [
    "*`tf.nn.sparse_softmax_cross_entropy_with_logits()`* computes the cross entropy based on the “logits” (i.e., the output of the network before going through the softmax activation function), and it expects labels in the form of integers ranging from 0 to the number of classes minus 1 (in our case, from 0 to 9). This will give us a 1D tensor containing the cross entropy for each instance. We can then use TensorFlow’s *`reduce_mean()`* function to compute the mean cross entropy over all instances. \n",
    "\n",
    "We also wish to estimate the accuracy of our model. For this you can use the [*`in_top_k()`* function](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) with *k=1*. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network’s overall accuracy.\n",
    "\n",
    "Now we need to define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AaR4LHcBd4M7"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hGNrKv2d4M8"
   },
   "source": [
    "Finally, we need to create a node to initialize all variables, and we will also create a *`Saver`* to\n",
    "save our trained model parameters to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k51TV_tFd4M9"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSdOjTUQd4NA"
   },
   "source": [
    "**Let's review the construction phase a little bit. We have:**\n",
    "- Created placeholders for the inputs and the targets;\n",
    "- Ceated a function to build a neuron layer and used it to create the DNN;\n",
    "- Defined the cost function and performance measure; and\n",
    "- Created an optimizer.\n",
    "\n",
    "Now move onto the execution phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu9NhbkNd4NB"
   },
   "source": [
    "<a id = \"2_2\"></a>\n",
    "\n",
    "## 2.2. Execution phase\n",
    "\n",
    "We first define the number of epochs that we want to run, as well as the size of the minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmzmGF_id4NC"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bwcvxj0sd4NI"
   },
   "source": [
    "We can check our training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MSq83Nkzd4NI",
    "outputId": "349d25ed-5dcd-438b-c34b-5d5abc0bc53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9vRuCatd4NK"
   },
   "source": [
    "Now we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I-qyx-V8d4NL",
    "outputId": "90e5a32c-c982-4906-820d-9ca758fb3f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain accuracy\tTest accuracy\n",
      "0\t0.9110181927680969\t0.914900004863739\n",
      "1\t0.9300545454025269\t0.9308000206947327\n",
      "2\t0.9415090680122375\t0.9416999816894531\n",
      "3\t0.948127269744873\t0.9484000205993652\n",
      "4\t0.9532363414764404\t0.9528999924659729\n",
      "5\t0.9573636651039124\t0.9555000066757202\n",
      "6\t0.9619454741477966\t0.9610999822616577\n",
      "7\t0.9655636548995972\t0.9617999792098999\n",
      "8\t0.9676545262336731\t0.9635000228881836\n",
      "9\t0.9704181551933289\t0.9645000100135803\n",
      "10\t0.9715454578399658\t0.9660999774932861\n",
      "11\t0.9742545485496521\t0.9675999879837036\n",
      "12\t0.975745439529419\t0.9682999849319458\n",
      "13\t0.977545440196991\t0.9689000248908997\n",
      "14\t0.9788545370101929\t0.9686999917030334\n",
      "15\t0.9798545241355896\t0.9700000286102295\n",
      "16\t0.9799090623855591\t0.9700999855995178\n",
      "17\t0.9822545647621155\t0.9710000157356262\n",
      "18\t0.9832545518875122\t0.9718999862670898\n",
      "19\t0.9846363663673401\t0.9726999998092651\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={x: mnist.train.images, y: mnist.train.labels})\n",
    "        acc_test = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"{}\\t{}\\t{}\".format(epoch, acc_train, acc_test))\n",
    "\n",
    "    save_path = saver.save(sess, \"models/example03/dnn_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaRoOYBEd4NN"
   },
   "source": [
    "If the folder containing this notebook does not contain `models` folder (which is the parent folder in the last line of code), you will get the error \n",
    "\n",
    "ValueError: ``Parent directory of models/example03/dnn_final.ckpt doesn't exist, can't save.``\n",
    "\n",
    "You **must** create `models` in the folder containing this notebook to fix the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn0_6aqRd4NN"
   },
   "source": [
    "<a id = \"2_3\"></a>\n",
    "\n",
    "## 2.3. Combining two phases using API functions of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvewp4Oyd4NN"
   },
   "source": [
    "As you might expect, TensorFlow comes with many handy functions to create standard neural network layers, so there’s often no need to define your own *`neuron_layer()`* function like we just did. For example, *`tf.layers.dense()`* function creates a fully connected layer, where all the inputs are connected to all the neurons in the layer. It takes care of creating the weights and biases variables, and it set the activation argument to *`None`*, but we can set it to activation functions such as *`tf.nn.relu`*. Let’s tweak the preceding code to use the *`tf.layers.dense()`* function instead of our *`neuron_layer()`* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9RIfDl4d4NO"
   },
   "source": [
    "* **Building a DNN model using predefined functions in TensorFlow**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUhing7xd4NO"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_inputs = 28 * 28\n",
    "num_hidden1 = 300\n",
    "num_hidden2 = 100\n",
    "num_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"x\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(x, num_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, num_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, num_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss)    \n",
    "    training_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name + \"/values\", var)\n",
    "        \n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "\n",
    "\n",
    "# summary\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# merge all summary\n",
    "tf.summary.histogram('hidden1/activations', hidden1)\n",
    "tf.summary.histogram('hidden2/activations', hidden2)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs/example03/dnn_final\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logdir + 'train', tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(logdir + 'test', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ae9Ej-Nfd4NQ"
   },
   "source": [
    "* **Training and Testing (Execution) the DNN model defined in the previous step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlnuTdZ-d4NR",
    "outputId": "2d390feb-ee90-48eb-8713-c616c6eee8e9"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n",
    "            \n",
    "        train_summary, acc_train = sess.run([merged, accuracy],\n",
    "                                             feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "        test_summary, acc_test = sess.run([accuracy_summary, accuracy],\n",
    "                                          feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        train_writer.add_summary(train_summary, epoch)\n",
    "        test_writer.add_summary(test_summary, epoch)\n",
    "\n",
    "        print(\"{}\\t{}\\t{}\".format(epoch, acc_train, acc_test))   \n",
    "        \n",
    "    save_path = saver.save(sess, \"models/example03/dnn_final.ckpt\")\n",
    "    \n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw8u19-9d4NT"
   },
   "source": [
    "<a id = \"2_4\"></a>\n",
    "\n",
    "## 2.4 Excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fz7rC18Qd4NT"
   },
   "source": [
    "- Change five different value for the number of hidden nodes in each layer in construction step (B.1) and report the best numbers among your chosen number.\n",
    "- Increase the number of hidden layers to **three** and set five values for the number of hidden nodes then report the best value and its performance.\n",
    "- Try to change the optimizer to train the model to [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjH-fp9ud4NT"
   },
   "source": [
    "---\n",
    "### <div  style=\"text-align:center\">**THE END**</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "CYB80001-Week2D-TensorFlow-II.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
