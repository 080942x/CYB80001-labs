{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYB80001 System Security Project\n",
    "Prepared by **Derui (Derek) Wang**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3A - Deep Feedforward Neural Networks with Keras\n",
    "\n",
    "**The purpose of this session is to demonstrate how to use TensorFlow's implementation of the Keras api to develop machine learning algorithms and deep neural network models. **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat Session 2D with Keras and build a simple softmax classification with MNIST dataset.\n",
    "\n",
    "* #### <span style=\"color:#0b486b\">Step 1: Load or download the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1011 23:37:46.087104 12848 deprecation.py:323] From <ipython-input-5-c72d886d7488>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1011 23:37:46.087104 12848 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1011 23:37:46.110110 12848 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W1011 23:37:52.342521 12848 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting datasets/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 23:37:52.969605 12848 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 23:37:54.059501 12848 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2: Build a sequential model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 6, you saw the following TensorFlow code to construct a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_inputs = 28 * 28\n",
    "num_hidden1 = 300\n",
    "num_hidden2 = 100\n",
    "num_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"x\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(x, num_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, num_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, num_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss)    \n",
    "    training_op = optimizer.apply_gradients(grads)    \n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))   \n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the Keras code to define such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 23:37:54.375393 12848 deprecation.py:506] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Adds a densely-connected layer with n_hidden1 units to the model:\n",
    "model.add(keras.layers.Dense(n_hidden1, activation='relu'))\n",
    "\n",
    "# Add another densely-connected layer with n_hidden2 units :\n",
    "model.add(keras.layers.Dense(n_hidden2, activation='relu'))\n",
    "\n",
    "# Add a softmax layer with n_outputs output units:\n",
    "model.add(keras.layers.Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "\n",
    "# Configure the learning process, including specifying the loss the evaluation metrics\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3: Train and test the model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the following code for training can be replaced.\n",
    "``` \n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n",
    "            \n",
    "        acc_train = sess.run(accuracy,\n",
    "                             feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "        acc_test = sess.run(accuracy,\n",
    "                             feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(\"{}\\t{}\\t{}\".format(epoch, acc_train))   \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is simple with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.7254 - sparse_categorical_accuracy: 0.8203 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.9066\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.3280 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2776 - val_sparse_categorical_accuracy: 0.9216\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.2737 - sparse_categorical_accuracy: 0.9220 - val_loss: 0.2419 - val_sparse_categorical_accuracy: 0.9296\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.2406 - sparse_categorical_accuracy: 0.9315 - val_loss: 0.2184 - val_sparse_categorical_accuracy: 0.9382\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2161 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.2021 - val_sparse_categorical_accuracy: 0.9425\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.1962 - sparse_categorical_accuracy: 0.9435 - val_loss: 0.1839 - val_sparse_categorical_accuracy: 0.9462\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.1800 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.1725 - val_sparse_categorical_accuracy: 0.9497\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.1660 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.1611 - val_sparse_categorical_accuracy: 0.9517\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.1536 - sparse_categorical_accuracy: 0.9569 - val_loss: 0.1541 - val_sparse_categorical_accuracy: 0.9546\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.1434 - sparse_categorical_accuracy: 0.9596 - val_loss: 0.1433 - val_sparse_categorical_accuracy: 0.9586\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.1338 - sparse_categorical_accuracy: 0.9625 - val_loss: 0.1359 - val_sparse_categorical_accuracy: 0.9603\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 0.1256 - sparse_categorical_accuracy: 0.9645 - val_loss: 0.1300 - val_sparse_categorical_accuracy: 0.9618\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 0.1183 - sparse_categorical_accuracy: 0.9664 - val_loss: 0.1241 - val_sparse_categorical_accuracy: 0.9630\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.1113 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.1226 - val_sparse_categorical_accuracy: 0.9643\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.1050 - sparse_categorical_accuracy: 0.9705 - val_loss: 0.1158 - val_sparse_categorical_accuracy: 0.9661\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0996 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1088 - val_sparse_categorical_accuracy: 0.9689\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0945 - sparse_categorical_accuracy: 0.9735 - val_loss: 0.1063 - val_sparse_categorical_accuracy: 0.9695\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0898 - sparse_categorical_accuracy: 0.9749 - val_loss: 0.1027 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0851 - sparse_categorical_accuracy: 0.9763 - val_loss: 0.1020 - val_sparse_categorical_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0811 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0984 - val_sparse_categorical_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13dadfc20b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(mnist.train.images, \n",
    "          mnist.train.labels, \n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=( mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An added benefit of using Keras is that the `model` Python variable preserves all the information regarding the model, including the current weights. So we can easily further improve the model with additional training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0773 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9719\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0738 - sparse_categorical_accuracy: 0.9802 - val_loss: 0.0925 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0707 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0910 - val_sparse_categorical_accuracy: 0.9727\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0674 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0881 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.0645 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0900 - val_sparse_categorical_accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(mnist.train.images, \n",
    "                    mnist.train.labels,\n",
    "                    epochs=5,\n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=( mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change five different value for the number of hidden nodes in each layer and report the best numbers among your chosen number.\n",
    "- Increase the number of hidden layers to **three** and set five values for the number of hidden nodes then report the best value and its performance.\n",
    "- Try to change the optimizer to train the model to [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Saving and loading Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 1: Build a CNN using Keras Model API</span>\n",
    "\n",
    "We first build a convolutional neural network using **Keras Model API**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Activation, Multiply, BatchNormalization\n",
    "from keras.layers import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 1: Building model using the Model API</span>\n",
    "\n",
    "We build the CNN model by using a function named `MNIST_CNN`. We do not use the sequential model here. Instead, we apply the layers from Keras `layers` module as functions and then use the `Model` API to wrap the layers into a model. \n",
    "\n",
    "The model is then compiled with a proper loss functions and an optimizer used for training. We also define the metric used for monitoring the performance on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_CNN(In_Shape):\n",
    "    inputs = Input(shape=In_Shape, name='Normal_inputs')\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C1')(inputs)\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C2')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), name='MP1')(x)\n",
    "    x = Convolution2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='C3')(x)\n",
    "    x = Convolution2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', name='C4')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), name='MP2')(x)\n",
    "    x = Flatten(name='Flatten')(x)\n",
    "    x = Dense(200, activation='relu', name='D1')(x)\n",
    "    x = Dense(200, activation='relu', name='D2')(x)\n",
    "    x = Dense(10, name='logits')(x)\n",
    "    outputs = Activation('softmax', name='normal_output')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2: Train the CNN</span>\n",
    "\n",
    "The CNN is taking inputs in the sahpe of **Height x Weight x Channel (HWC)**. Hence, we first need to reshape each example to shape (28,28,1). In other words, we can reshape the entire dataset to **(-1,28,28,1)**. \n",
    "\n",
    "Next, we convert the labels into one-hot-vectors for training using non-sparese loss functions. WE can use the `to_categorical` function from `keras.utils` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1)\n",
      "(55000, 10)\n",
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 9s 156us/step - loss: 0.1281 - acc: 0.9590 - val_loss: 0.0393 - val_acc: 0.9872\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0409 - acc: 0.9872 - val_loss: 0.0312 - val_acc: 0.9900\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0288 - acc: 0.9910 - val_loss: 0.0353 - val_acc: 0.9898\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 8s 140us/step - loss: 0.0202 - acc: 0.9937 - val_loss: 0.0244 - val_acc: 0.9923\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 8s 150us/step - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0291 - val_acc: 0.9906\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 8s 147us/step - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0264 - val_acc: 0.9921\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 8s 147us/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0294 - val_acc: 0.9919\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 8s 144us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0280 - val_acc: 0.9928\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0099 - acc: 0.9972 - val_loss: 0.0316 - val_acc: 0.9920\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 8s 144us/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0305 - val_acc: 0.9931\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 8s 144us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0358 - val_acc: 0.9918\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 8s 145us/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0289 - val_acc: 0.9936\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 8s 145us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0313 - val_acc: 0.9919\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0436 - val_acc: 0.9918\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 8s 144us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0343 - val_acc: 0.9924\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0362 - val_acc: 0.9931\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 8s 145us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0415 - val_acc: 0.9926\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 8s 146us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0421 - val_acc: 0.9917\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 8s 147us/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0457 - val_acc: 0.9909\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 8s 148us/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0437 - val_acc: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1402d58a828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "mnist_train_images = mnist.train.images.reshape((-1,28,28,1))\n",
    "mnist_train_labels = to_categorical(mnist.train.labels)\n",
    "mnist_test_images = mnist.test.images.reshape((-1,28,28,1))\n",
    "mnist_test_labels = to_categorical(mnist.test.labels)\n",
    "print(mnist_train_images.shape)\n",
    "print(mnist_train_labels.shape)\n",
    "\n",
    "mnist_cnn = MNIST_CNN(mnist_train_images[0].shape)\n",
    "mnist_cnn.fit(mnist_train_images, \n",
    "              mnist_train_labels,\n",
    "              epochs=num_epochs,\n",
    "              batch_size=batch_size, \n",
    "              validation_data=(mnist_test_images, mnist_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3: Save the trained model</span>\n",
    "\n",
    "We can simple call the `save` method on the trained model to save it. We recommend saving the model using `.h5` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn.save('mnist_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 4: Load pre-trained model</span>\n",
    "\n",
    "To load a pre-trained Keras model, we can use the `load_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the loaded model\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "Accuracy of the loaded model: 0.991\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('mnist_cnn_model.h5')\n",
    "print('Evaluate the loaded model')\n",
    "print(f'Accuracy of the loaded model: {loaded_model.evaluate(mnist_test_images, mnist_test_labels)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <div  style=\"text-align:center\">**THE END**</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
