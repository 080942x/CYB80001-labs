{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYB80001 System Security Project\n",
    "Prepared by **Derui (Derek) Wang**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3A - Deep Feedforward Neural Networks with Keras\n",
    "\n",
    "**The purpose of this session is to demonstrate how to use TensorFlow's implementation of the Keras api to develop machine learning algorithms and deep neural network models. **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "\n",
    "### Part 1 Classification with Keras\n",
    "\n",
    "### Part 2 Part 2. Saving and loading Keras models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat Session 2D with Keras and build a simple softmax classification with MNIST dataset.\n",
    "\n",
    "* #### <span style=\"color:#0b486b\">Step 1: Load or download the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1013 17:40:23.510822  9656 deprecation.py:323] From <ipython-input-4-c72d886d7488>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1013 17:40:23.510822  9656 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1013 17:40:23.512793  9656 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 17:40:23.855744  9656 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 17:40:24.628719  9656 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2: Build a sequential model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 6, you saw the following TensorFlow code to construct a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_inputs = 28 * 28\n",
    "num_hidden1 = 300\n",
    "num_hidden2 = 100\n",
    "num_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"x\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(x, num_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, num_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, num_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss)    \n",
    "    training_op = optimizer.apply_gradients(grads)    \n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))   \n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the Keras code to define such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 17:40:24.837246  9656 deprecation.py:506] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Adds a densely-connected layer with n_hidden1 units to the model:\n",
    "model.add(keras.layers.Dense(n_hidden1, activation='relu'))\n",
    "\n",
    "# Add another densely-connected layer with n_hidden2 units :\n",
    "model.add(keras.layers.Dense(n_hidden2, activation='relu'))\n",
    "\n",
    "# Add a softmax layer with n_outputs output units:\n",
    "model.add(keras.layers.Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "\n",
    "# Configure the learning process, including specifying the loss the evaluation metrics\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3: Train and test the model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the following code for training can be replaced.\n",
    "``` \n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n",
    "            \n",
    "        acc_train = sess.run(accuracy,\n",
    "                             feed_dict={x: x_batch, y: y_batch})\n",
    "        \n",
    "        acc_test = sess.run(accuracy,\n",
    "                             feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        \n",
    "        print(\"{}\\t{}\\t{}\".format(epoch, acc_train))   \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is simple with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 0.7576 - sparse_categorical_accuracy: 0.8052 - val_loss: 0.3568 - val_sparse_categorical_accuracy: 0.9039\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.3321 - sparse_categorical_accuracy: 0.9072 - val_loss: 0.2784 - val_sparse_categorical_accuracy: 0.9212\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.2753 - sparse_categorical_accuracy: 0.9210 - val_loss: 0.2407 - val_sparse_categorical_accuracy: 0.9319\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.2408 - sparse_categorical_accuracy: 0.9307 - val_loss: 0.2151 - val_sparse_categorical_accuracy: 0.9384\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2151 - sparse_categorical_accuracy: 0.9376 - val_loss: 0.1952 - val_sparse_categorical_accuracy: 0.9448\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.1946 - sparse_categorical_accuracy: 0.9436 - val_loss: 0.1825 - val_sparse_categorical_accuracy: 0.9463\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.1774 - sparse_categorical_accuracy: 0.9487 - val_loss: 0.1685 - val_sparse_categorical_accuracy: 0.9504\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.1631 - sparse_categorical_accuracy: 0.9531 - val_loss: 0.1559 - val_sparse_categorical_accuracy: 0.9537\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.1506 - sparse_categorical_accuracy: 0.9565 - val_loss: 0.1472 - val_sparse_categorical_accuracy: 0.9560\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.1399 - sparse_categorical_accuracy: 0.9597 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9599\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.1304 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.1304 - val_sparse_categorical_accuracy: 0.9609\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.1217 - sparse_categorical_accuracy: 0.9659 - val_loss: 0.1260 - val_sparse_categorical_accuracy: 0.9620\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.1145 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.1182 - val_sparse_categorical_accuracy: 0.9644\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.1074 - sparse_categorical_accuracy: 0.9697 - val_loss: 0.1140 - val_sparse_categorical_accuracy: 0.9654\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.1013 - sparse_categorical_accuracy: 0.9716 - val_loss: 0.1115 - val_sparse_categorical_accuracy: 0.9665\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0958 - sparse_categorical_accuracy: 0.9732 - val_loss: 0.1052 - val_sparse_categorical_accuracy: 0.9677\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.0904 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9685\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.0855 - sparse_categorical_accuracy: 0.9757 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9701\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.0811 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.0985 - val_sparse_categorical_accuracy: 0.9699\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.0773 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.0928 - val_sparse_categorical_accuracy: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287588da748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(mnist.train.images, \n",
    "          mnist.train.labels, \n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=( mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An added benefit of using Keras is that the `model` Python variable preserves all the information regarding the model, including the current weights. So we can easily further improve the model with additional training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.0736 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.0930 - val_sparse_categorical_accuracy: 0.9708\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.0700 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0895 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.0666 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.0884 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.0634 - sparse_categorical_accuracy: 0.9828 - val_loss: 0.0865 - val_sparse_categorical_accuracy: 0.9735\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.0606 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.0868 - val_sparse_categorical_accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(mnist.train.images, \n",
    "                    mnist.train.labels,\n",
    "                    epochs=5,\n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=( mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change five different value for the number of hidden nodes in each layer and report the best numbers among your chosen number.\n",
    "- Increase the number of hidden layers to **three** and set five values for the number of hidden nodes then report the best value and its performance.\n",
    "- Try to change the optimizer to train the model to [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Saving and loading Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 1: Build a CNN using Keras Model API</span>\n",
    "\n",
    "We first build a convolutional neural network using **Keras Model API**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Activation, Multiply, BatchNormalization\n",
    "from keras.layers import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 1: Building model using the Model API</span>\n",
    "\n",
    "We build the CNN model by using a function named `MNIST_CNN`. We do not use the sequential model here. Instead, we apply the layers from Keras `layers` module as functions and then use the `Model` API to wrap the layers into a model. \n",
    "\n",
    "The model is then compiled with a proper loss functions and an optimizer used for training. We also define the metric used for monitoring the performance on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_CNN(In_Shape):\n",
    "    inputs = Input(shape=In_Shape, name='Normal_inputs')\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C1')(inputs)\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C2')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), name='MP1')(x)\n",
    "    x = Flatten(name='Flatten')(x)\n",
    "    x = Dense(200, activation='relu', name='D1')(x)\n",
    "    x = Dense(10, name='logits')(x)\n",
    "    outputs = Activation('softmax', name='normal_output')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 2: Train the CNN</span>\n",
    "\n",
    "The CNN is taking inputs in the sahpe of **Height x Weight x Channel (HWC)**. Hence, we first need to reshape each example to shape (28,28,1). In other words, we can reshape the entire dataset to **(-1,28,28,1)**. \n",
    "\n",
    "Next, we convert the labels into one-hot-vectors for training using non-sparese loss functions. WE can use the `to_categorical` function from `keras.utils` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 17:41:25.953994  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1013 17:41:25.956986  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1013 17:41:25.977958  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1013 17:41:26.002887  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1013 17:41:26.003861  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1013 17:41:26.021842  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1013 17:41:26.090657  9656 deprecation.py:323] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1013 17:41:26.138531  9656 deprecation_wrapper.py:119] From c:\\users\\windows\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1)\n",
      "(55000, 10)\n",
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 9s 155us/step - loss: 0.1364 - acc: 0.9589 - val_loss: 0.0576 - val_acc: 0.9810\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0405 - acc: 0.9873 - val_loss: 0.0429 - val_acc: 0.9856\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 7s 132us/step - loss: 0.0262 - acc: 0.9919 - val_loss: 0.0469 - val_acc: 0.9854\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0398 - val_acc: 0.9872\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0135 - acc: 0.9954 - val_loss: 0.0387 - val_acc: 0.9882\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0390 - val_acc: 0.9903\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0418 - val_acc: 0.9884\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0388 - val_acc: 0.9899\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 7s 129us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0549 - val_acc: 0.9893\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0514 - val_acc: 0.9879\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0456 - val_acc: 0.9902\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0491 - val_acc: 0.9901\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 7s 132us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0531 - val_acc: 0.9897\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0597 - val_acc: 0.9887\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0561 - val_acc: 0.9889\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0548 - val_acc: 0.9902\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0479 - val_acc: 0.9916\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 7s 130us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0717 - val_acc: 0.9871\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 7s 131us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0551 - val_acc: 0.9901\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 7s 132us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0524 - val_acc: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287425c2208>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "mnist_train_images = mnist.train.images.reshape((-1,28,28,1))\n",
    "mnist_train_labels = to_categorical(mnist.train.labels)\n",
    "mnist_test_images = mnist.test.images.reshape((-1,28,28,1))\n",
    "mnist_test_labels = to_categorical(mnist.test.labels)\n",
    "print(mnist_train_images.shape)\n",
    "print(mnist_train_labels.shape)\n",
    "\n",
    "mnist_cnn = MNIST_CNN(mnist_train_images[0].shape)\n",
    "mnist_cnn.fit(mnist_train_images, \n",
    "              mnist_train_labels,\n",
    "              epochs=num_epochs,\n",
    "              batch_size=batch_size, \n",
    "              validation_data=(mnist_test_images, mnist_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 3: Save the trained model</span>\n",
    "\n",
    "We can simple call the `save` method on the trained model to save it. We recommend saving the model using `.h5` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn.save('mnist_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### <span style=\"color:#0b486b\">Step 4: Load pre-trained model</span>\n",
    "\n",
    "To load a pre-trained Keras model, we can use the `load_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the loaded model\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "Accuracy of the loaded model: 0.9916\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('mnist_cnn_model.h5')\n",
    "print('Evaluate the loaded model')\n",
    "print(f'Accuracy of the loaded model: {loaded_model.evaluate(mnist_test_images, mnist_test_labels)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <div  style=\"text-align:center\">**THE END**</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
